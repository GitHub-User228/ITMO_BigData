{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "religious-browser",
   "metadata": {},
   "outputs": [],
   "source": [
    "USERNAME = \"ajufrjakova-372071\"\n",
    " \n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "#from pymorphy2 import MorphAnalyzer\n",
    "import os\n",
    "import socket\n",
    "from pyspark import SparkConf, SparkContext, SQLContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, length, when, col\n",
    "from pyspark.sql.types import BooleanType, IntegerType, LongType, StringType, ArrayType, FloatType, StructType, StructField\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import types as T\n",
    "from pyspark.sql.functions import pandas_udf\n",
    "from pyspark.sql.functions import PandasUDFType\n",
    "from jinja2 import Environment, FileSystemLoader\n",
    " \n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"/opt/conda/bin/python3.8\"\n",
    "SPARK_ADDRESS = \"local[4]\"\n",
    "LOCAL_IP = socket.gethostbyname(socket.gethostname())\n",
    " \n",
    "APP_NAME = \"practice_current\"\n",
    " \n",
    " \n",
    "# run spark\n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .appName(APP_NAME)\\\n",
    "    .master(SPARK_ADDRESS)\\\n",
    "    .config('spark.ui.port', \"4040\")\\\n",
    "    .config(\"spark.memory.fraction\", \"0.8\")\\\n",
    "    .config(\"spark.memory.storageFraction\", \"0.6\")\\\n",
    "    .config(\"spark.driver.memory\", \"4g\")\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Loading splitted data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "collected-tunnel",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_df = spark.read.parquet('hdfs:///project-dirs/2023-14000/split_data/summary.parquet')\n",
    "\n",
    "title_df = spark.read.parquet('hdfs:///project-dirs/2023-14000/split_data/title.parquet')\n",
    "\n",
    "main_df = spark.read.parquet('hdfs:///project-dirs/2023-14000/split_data/main.parquet')\n",
    "\n",
    "link_df = spark.read.parquet('hdfs:///project-dirs/2023-14000/split_data/link.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "similar-pioneer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- text: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "link_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Preprocessing titles and summaries"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dying-entertainment",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocess(df: \"pyspark.sql.dataframe.DataFrame\",\n",
    "           F: \"pyspark.sql.functions\",\n",
    "           T: \"pyspark.sql.types\",\n",
    "           broadcast_func: \"spark.sparkContext.broadcast\") -> 'Tuple[\"pyspark.sql.dataframe.DataFrame\"]':\n",
    "    \n",
    "    stop_words = nltk.corpus.stopwords.words(\"russian\")\n",
    "    stop_words_dict = broadcast_func(stop_words)\n",
    "    \n",
    "    @F.udf(returnType=T.StringType())\n",
    "    def preproc(text):\n",
    "        # remove numbers, extra spaces and some non-alphabetic characters\n",
    "        text = text.replace('%', ' процент')\n",
    "        text = text.replace('&quot', ' ')\n",
    "        text = re.sub('й', 'й', text)\n",
    "        text = re.sub('Й', 'Й', text)\n",
    "        text = re.sub(\"[^0-9A-Za-zа-яА-ЯёЁ ]\", \" \", text)\n",
    "        text = re.sub(' +', ' ', text).lstrip()\n",
    "        # convert to lower case\n",
    "        text = text.lower()\n",
    "        text = text.split(\" \")\n",
    "        # remove stop words and lemmatization\n",
    "        #text = [MorphAnalyzer().normal_forms(token)[0] for token in text]\n",
    "        text = [word for word in text if not word in stop_words]\n",
    "        text = \" \".join(text)\n",
    "        return text\n",
    "    \n",
    "    modified_df = df.where(F.col('text').isNotNull()).withColumn('text_proc', preproc('text'))\\\n",
    "    .select('id', 'text_proc')\n",
    "\n",
    "    \n",
    "    return modified_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "outdoor-prior",
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc_summary_df = text_preprocess(\n",
    "    df=summary_df,\n",
    "    F=F,\n",
    "    T=T,\n",
    "    broadcast_func=spark.sparkContext.broadcast\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "capable-plymouth",
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc_title_df = text_preprocess(\n",
    "    df=title_df,\n",
    "    F=F,\n",
    "    T=T,\n",
    "    broadcast_func=spark.sparkContext.broadcast\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "persistent-subject",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+\n",
      "|  id|           text_proc|\n",
      "+----+--------------------+\n",
      "|ID_0|                    |\n",
      "|ID_1|барселона договор...|\n",
      "|ID_2|временное правите...|\n",
      "|ID_3|газпром выставил ...|\n",
      "|ID_4|дочка французской...|\n",
      "|ID_5|единая россия нам...|\n",
      "|ID_6|единая россия пре...|\n",
      "|ID_7|зенит обжаловал р...|\n",
      "|ID_8|локомотив своем о...|\n",
      "|ID_9|локомотив одержал...|\n",
      "+----+--------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+----+--------------------+\n",
      "|  id|           text_proc|\n",
      "+----+--------------------+\n",
      "|ID_0|                    |\n",
      "|ID_1|12 виолончелистов...|\n",
      "|ID_2|24 часа ле мана и...|\n",
      "|ID_3|2morrow завтра мо...|\n",
      "|ID_4|8 первых свиданий...|\n",
      "|ID_5|a e a a a a c n i...|\n",
      "|ID_6|coca cola поделил...|\n",
      "|ID_7|do it гараже 400 ...|\n",
      "|ID_8|forward us цукерб...|\n",
      "|ID_9|gm автоваз повыси...|\n",
      "+----+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preproc_summary_df.show(10)\n",
    "preproc_title_df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Preprocessing types"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "extraordinary-virgin",
   "metadata": {},
   "outputs": [],
   "source": [
    "def type_preprocess(df: \"pyspark.sql.dataframe.DataFrame\",\n",
    "           F: \"pyspark.sql.functions\",\n",
    "           T: \"pyspark.sql.types\",\n",
    "           broadcast_func: \"spark.sparkContext.broadcast\") -> 'Tuple[\"pyspark.sql.dataframe.DataFrame\"]':\n",
    "    \n",
    "    stop_words = nltk.corpus.stopwords.words(\"russian\")\n",
    "    stop_words_dict = broadcast_func(stop_words)\n",
    "    \n",
    "    @F.udf(returnType=T.StringType())\n",
    "    def preproc(text):\n",
    "        # remove numbers, extra spaces and some non-alphabetic characters\n",
    "        text = re.sub('й', 'й', text)\n",
    "        text = re.sub('Й', 'Й', text)\n",
    "        text = re.sub(\"[^0-9A-Za-zа-яА-ЯёЁ ]\", \" \", text)\n",
    "        text = re.sub(' +', ' ', text).lstrip()\n",
    "        # convert to lower case\n",
    "        text = text.lower()\n",
    "        text = text.split(\" \")\n",
    "        # remove stop words and lemmatization\n",
    "        #text = [MorphAnalyzer().normal_forms(token)[0] for token in text]\n",
    "        text = [word for word in text if not word in stop_words]\n",
    "        text = \" \".join(text)\n",
    "        return text\n",
    "    \n",
    "    modified_df = df.where(F.col('type').isNotNull()).withColumn('type_proc', preproc('type'))\n",
    "\n",
    "    \n",
    "    return modified_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bibliographic-stack",
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc_type_df = type_preprocess(\n",
    "    df=main_df,\n",
    "    F=F,\n",
    "    T=T,\n",
    "    broadcast_func=spark.sparkContext.broadcast\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Preprocessing date_parsed"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "outdoor-pound",
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_preprocess(df: \"pyspark.sql.dataframe.DataFrame\",\n",
    "           F: \"pyspark.sql.functions\",\n",
    "           T: \"pyspark.sql.types\") -> 'Tuple[\"pyspark.sql.dataframe.DataFrame\"]':\n",
    "    \n",
    "    @F.udf(returnType=T.StringType())\n",
    "    def preproc_date(input_date):\n",
    "        # performs preprocessing of date input_date as string\n",
    "        try:\n",
    "            output_date = re.sub(r'\\D', '/', input_date).split('/')\n",
    "            if len(output_date[2]) == 4:\n",
    "                output_date[0], output_date[2] = output_date[2], output_date[0]\n",
    "            output_date = '/'.join(output_date)\n",
    "        except:\n",
    "            output_date = None\n",
    "        return output_date\n",
    "    \n",
    "    modified_df = df.where(F.col('date_parsed').isNotNull()).withColumn('date_parsed_proc', preproc_date('date_parsed'))\\\n",
    "\n",
    "    \n",
    "    return modified_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "perfect-start",
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc_main_df = date_preprocess(\n",
    "    df=preproc_type_df,\n",
    "    F=F,\n",
    "    T=T\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "velvet-diversity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+------+-----------+----------+---------+----------+------------+----------------+\n",
      "|        type|      date|source|date_parsed|  title_id|  link_id|summary_id|   type_proc|date_parsed_proc|\n",
      "+------------+----------+------+-----------+----------+---------+----------+------------+----------------+\n",
      "|    ОБЩЕСТВО|2011-10-27|   aif| 2011-10-27|ID_1071283|ID_815561|ID_1353702|    общество|      2011/10/27|\n",
      "|ПРОИСШЕСТВИЯ|2011-10-27|   aif| 2011-10-27|ID_1811736|  ID_8540|ID_2353835|происшествия|      2011/10/27|\n",
      "|    ОБЩЕСТВО|2011-10-27|   aif| 2011-10-27| ID_814905|ID_557641|ID_2304122|    общество|      2011/10/27|\n",
      "|    ОБЩЕСТВО|2011-10-27|   aif| 2011-10-27|ID_1141948|ID_557640|ID_1366343|    общество|      2011/10/27|\n",
      "|ПРОИСШЕСТВИЯ|2011-10-27|   aif| 2011-10-27|ID_1763618| ID_87843|ID_1908113|происшествия|      2011/10/27|\n",
      "|ПРОИСШЕСТВИЕ|2011-10-27|   aif| 2011-10-27| ID_982516|ID_245197| ID_744592|происшествие|      2011/10/27|\n",
      "|       СПОРТ|2011-10-27|   aif| 2011-10-27|ID_1301328|ID_237707|ID_3030439|       спорт|      2011/10/27|\n",
      "|    ОБЩЕСТВО|2011-10-27|   aif| 2011-10-27|ID_3362772|ID_356029|ID_3126970|    общество|      2011/10/27|\n",
      "|    КУЛЬТУРА|2011-10-27|   aif| 2011-10-27|ID_3326083|ID_584280|ID_1788174|    культура|      2011/10/27|\n",
      "|       ГОРОД|2011-10-27|   aif| 2011-10-27|ID_3325347|ID_320600|ID_3131663|       город|      2011/10/27|\n",
      "+------------+----------+------+-----------+----------+---------+----------+------------+----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preproc_main_df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adult-algorithm",
   "metadata": {},
   "source": [
    "# Get unique tokens from titles, summaries and types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "blessed-vertical",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab(df: \"pyspark.sql.dataframe.DataFrame\",\n",
    "           F: \"pyspark.sql.functions\",\n",
    "           T: \"pyspark.sql.types\") -> 'Tuple[\"pyspark.sql.dataframe.DataFrame\"]':\n",
    "\n",
    "    def text_to_words(text):\n",
    "        words = [word for word in text.split(' ')]\n",
    "        return words\n",
    "\n",
    "    textToWords = udf(text_to_words, returnType=ArrayType(StringType()))\n",
    "    \n",
    "    word_counts_df = df\\\n",
    "    .where(col('text_proc').isNotNull())\\\n",
    "    .select(F.explode(textToWords('text_proc')).name('word'))\\\n",
    "    .groupby('word')\\\n",
    "    .count()\\\n",
    "    .orderBy('count', ascending=False)\n",
    "    \n",
    "    \n",
    "    return  word_counts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "accepting-track",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_types(df: \"pyspark.sql.dataframe.DataFrame\",\n",
    "           F: \"pyspark.sql.functions\",\n",
    "           T: \"pyspark.sql.types\") -> 'Tuple[\"pyspark.sql.dataframe.DataFrame\"]':\n",
    "    \n",
    "    word_counts_df = df\\\n",
    "    .where(col('type_proc').isNotNull())\\\n",
    "    .select('type')\\\n",
    "    .groupby('type')\\\n",
    "    .count()\\\n",
    "    .orderBy('count', ascending=False)\n",
    "    \n",
    "    \n",
    "    return  word_counts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "numeric-complexity",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_unique = get_vocab(\n",
    "    df=preproc_summary_df,\n",
    "    F=F,\n",
    "    T=T\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "lined-latest",
   "metadata": {},
   "outputs": [],
   "source": [
    "title_unique = get_vocab(\n",
    "    df=preproc_title_df,\n",
    "    F=F,\n",
    "    T=T\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "satellite-packaging",
   "metadata": {},
   "outputs": [],
   "source": [
    "types_unique = get_types(\n",
    "    df=preproc_main_df,\n",
    "    F=F,\n",
    "    T=T\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "certain-carroll",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+\n",
      "|   word|  count|\n",
      "+-------+-------+\n",
      "|       |2874810|\n",
      "| россии| 564478|\n",
      "|   nbsp| 329041|\n",
      "|    это| 321706|\n",
      "|   года| 286193|\n",
      "| заявил| 279331|\n",
      "|    сша| 268881|\n",
      "|     рф| 231744|\n",
      "|области| 206782|\n",
      "|  также| 199914|\n",
      "+-------+-------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+-------+------+\n",
      "|   word| count|\n",
      "+-------+------+\n",
      "|       |626017|\n",
      "| россии|263179|\n",
      "|    сша|199898|\n",
      "|     рф|116571|\n",
      "| москве| 90603|\n",
      "|  путин| 86329|\n",
      "|области| 77732|\n",
      "|человек| 69847|\n",
      "| против| 63380|\n",
      "| заявил| 61166|\n",
      "+-------+------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+--------------+-------+\n",
      "|          type|  count|\n",
      "+--------------+-------+\n",
      "|     Экономика|1537288|\n",
      "|      Политика| 937839|\n",
      "|        В мире| 748367|\n",
      "|      Общество| 739195|\n",
      "|Лента новостей| 698124|\n",
      "|           Мир| 649760|\n",
      "|         Спорт| 627528|\n",
      "|        Россия| 476762|\n",
      "|  Происшествия| 457470|\n",
      "|      Культура| 451611|\n",
      "+--------------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "summary_unique.show(10)\n",
    "title_unique.show(10)\n",
    "types_unique.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "lyric-verse",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1001161"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_unique.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "integrated-association",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "465575"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title_unique.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "burning-pepper",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12344"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "types_unique.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Lemmatization of vocabularies"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Skipped, because MorphAnalyzer package is not installed\n",
    "This part was performed locally and new vocabularies were pushed to cluster"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "id": "blond-handle",
   "metadata": {},
   "source": [
    "# Lematization of texts in titles, summaries and types using lemmatized vocabularies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "under-liabilities",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_voc = spark.read.csv('hdfs:///project-dirs/2023-14000/split_data/voc_summary.csv', header=True)\n",
    "\n",
    "title_voc = spark.read.csv('hdfs:///project-dirs/2023-14000/split_data/voc_title.csv', header=True)\n",
    "\n",
    "type_voc = spark.read.csv('hdfs:///project-dirs/2023-14000/split_data/types.csv', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dated-honor",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_voc = summary_voc.toPandas()\n",
    "sum_voc = dict(zip(sum_voc['word'], sum_voc['lemma']))\n",
    "\n",
    "ttl_voc = title_voc.toPandas()\n",
    "ttl_voc = dict(zip(ttl_voc['word'], ttl_voc['lemma']))\n",
    "\n",
    "tp_voc = type_voc.toPandas()\n",
    "tp_voc = dict(zip(tp_voc['type'], tp_voc['reformed_type']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boxed-queensland",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_lemma(df: \"pyspark.sql.dataframe.DataFrame\",\n",
    "           F: \"pyspark.sql.functions\",\n",
    "           T: \"pyspark.sql.types\",\n",
    "           text_voc: dict,\n",
    "           broadcast_func: \"spark.sparkContext.broadcast\") -> 'Tuple[\"pyspark.sql.dataframe.DataFrame\"]':\n",
    "\n",
    "    text_dict = broadcast_func(text_voc)    \n",
    "    \n",
    "    @F.udf(returnType=T.StringType())\n",
    "    def lemmatize_text(text):\n",
    "            text = text.split(\" \")\n",
    "            text = [text_dict.value.get(word, word) for word in text]\n",
    "            output_text = \" \".join(text)       \n",
    "            return output_text\n",
    "\n",
    "    modified_df = df.where(F.col('text_proc').isNotNull()).withColumn('lemmatized', lemmatize_text('text_proc'))\\\n",
    "    .select('id', 'text_proc','lemmatized')\n",
    "\n",
    "    \n",
    "    return modified_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proprietary-idaho",
   "metadata": {},
   "outputs": [],
   "source": [
    "def type_to_lemma(df: \"pyspark.sql.dataframe.DataFrame\",\n",
    "           F: \"pyspark.sql.functions\",\n",
    "           T: \"pyspark.sql.types\",\n",
    "           text_voc: dict,\n",
    "           broadcast_func: \"spark.sparkContext.broadcast\") -> 'Tuple[\"pyspark.sql.dataframe.DataFrame\"]':\n",
    "\n",
    "    text_dict = broadcast_func(text_voc)    \n",
    "    \n",
    "    @F.udf(returnType=T.StringType())\n",
    "    def lemmatize_text(text):\n",
    "            output_text = text_dict.value.get(text, text)      \n",
    "            return output_text\n",
    "\n",
    "    modified_df = df.where(F.col('type_proc').isNotNull()).withColumn('lemmatized', lemmatize_text('type_proc'))\\\n",
    "    .drop('date_parsed', 'type_proc', 'type')\\\n",
    "    .select('date', 'source' , 'title_id', 'link_id', 'summary_id', F.col('lemmatized').alias('type'), F.col('date_parsed_proc').alias('date_parsed'))\\\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    return modified_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "understanding-gabriel",
   "metadata": {},
   "outputs": [],
   "source": [
    "lem_summary_df = text_to_lemma(\n",
    "    df=preproc_summary_df,\n",
    "    F=F,\n",
    "    T=T,\n",
    "    text_voc = sum_voc,\n",
    "    broadcast_func=spark.sparkContext.broadcast\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sunset-store",
   "metadata": {},
   "outputs": [],
   "source": [
    "lem_title_df = text_to_lemma(\n",
    "    df=preproc_title_df,\n",
    "    F=F,\n",
    "    T=T,\n",
    "    text_voc = ttl_voc,\n",
    "    broadcast_func=spark.sparkContext.broadcast\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "several-wealth",
   "metadata": {},
   "outputs": [],
   "source": [
    "lem_main_df = type_to_lemma(\n",
    "    df=preproc_main_df,\n",
    "    F=F,\n",
    "    T=T,\n",
    "    text_voc = tp_voc,\n",
    "    broadcast_func=spark.sparkContext.broadcast\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interesting-republic",
   "metadata": {},
   "outputs": [],
   "source": [
    "lem_summary_df.show(10)\n",
    "lem_title_df.show(10)\n",
    "lem_main_df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "military-behalf",
   "metadata": {},
   "source": [
    "# Data Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "overall-ordinary",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib import rc_context"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Loading markers"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "summary = spark.read.csv('hdfs:///project-dirs/2023-14000/summary_marked.csv', header=True)\n",
    "title = spark.read.csv('hdfs:///project-dirs/2023-14000/title_marked.csv', header=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "id": "ignored-religion",
   "metadata": {},
   "source": [
    "## 1.Top words"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Skipped, because of memory issues"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.Sentiments Rates"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "@F.udf(returnType=T.IntegerType())\n",
    "def collect0(labels):\n",
    "    counts = Counter(labels)\n",
    "    if 0 not in counts.keys():\n",
    "        return 0\n",
    "    return counts[0]\n",
    "\n",
    "@F.udf(returnType=T.IntegerType())\n",
    "def collect1(labels):\n",
    "    counts = Counter(labels)\n",
    "    if 1 not in counts.keys():\n",
    "        return 0\n",
    "    return counts[1]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "@F.udf(returnType=T.StringType())\n",
    "def sources_to_groups(source):\n",
    "    pro_rus = ['aif', 'argumenti', 'fontanka', 'interfax', 'izvestia', 'kp', 'lenta', 'mail', 'pravda', 'rbc', 'regnum',\n",
    "               'rg', 'ria', 'rosbalt', 'tass', 'ura', 'vedomosti', 'vesti', 'yandex']\n",
    "    pro_opp = ['currenttime', 'mediazona', 'meduza', 'novyagazeta', 'svoboda', 'thebell', 'tvrain']\n",
    "    if source in pro_opp:\n",
    "        return 'pro_opposite'\n",
    "    else:\n",
    "        return 'pro_russian'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "def grouper(main, cols_to_group, max_groups=None):\n",
    "    grouped = main.select(*cols_to_group, 'label') \\\n",
    "                  .groupby(cols_to_group) \\\n",
    "                  .agg(collect0(F.col('label')).alias('neg_count'),\n",
    "                       collect1(F.col('label')).alias('pos_count'))\n",
    "    grouped = grouped.withColumn('total', grouped['neg_count'] + grouped['pos_count']) \\\n",
    "                     .orderBy('total', ascending=False)\n",
    "    if max_groups is not None:\n",
    "        grouped = grouped.limit(max_groups)\n",
    "    grouped = grouped.withColumn('neg_rate', grouped['neg_count'] / ( grouped['neg_count'] + grouped['pos_count'] )) \\\n",
    "                     .withColumn('pos_rate', grouped['pos_count'] / ( grouped['neg_count'] + grouped['pos_count'] ))\n",
    "    return grouped"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-18T10:29:52.042375300Z",
     "start_time": "2023-06-18T10:29:52.002676300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def reshape_df(df, cols_to_group):\n",
    "    df0 = df.select(*cols_to_group, 'neg_rate') \\\n",
    "            .withColumnRenamed('neg_rate', 'rate') \\\n",
    "            .orderBy('rate', ascending=True) \\\n",
    "            .withColumn('rate', F.col('rate')*100) \\\n",
    "            .withColumn('label', F.lit('negative')) \\\n",
    "            .toPandas()\n",
    "    df1 = df.select(*cols_to_group, 'pos_rate') \\\n",
    "            .withColumnRenamed('pos_rate', 'rate') \\\n",
    "            .orderBy('rate', ascending=True) \\\n",
    "            .withColumn('rate', F.col('rate')*100) \\\n",
    "            .withColumn('label', F.lit('non-negative')) \\\n",
    "            .toPandas()\n",
    "    df_res = pd.concat([df0, df1], axis=0, ignore_index=True)\n",
    "    return df_res"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "def custom_bar_plot(data, x, y, hue, fs=18, k=1, aspect_ratio=0.5, x_rotation=45,\n",
    "                    x_vals=None, show_values=False, ylim=(0, 100)):\n",
    "    sns.set_style(\"darkgrid\")\n",
    "    with rc_context({'font.size': fs * k}):\n",
    "        plt.figure(figsize=(k * 16, k * 16 * aspect_ratio))\n",
    "        if x_vals is not None:\n",
    "          plot = sns.barplot(data=data[data[x].isin(x_vals)], x=x, y=y, hue=hue)\n",
    "        else:\n",
    "          plot = sns.barplot(data=data, x=x, y=y, hue=hue)\n",
    "        if show_values:\n",
    "          for container in plot.containers:\n",
    "            plot.bar_label(container, fmt='%.0f')\n",
    "        plt.ylim(*ylim)\n",
    "        plt.xticks(rotation=x_rotation)\n",
    "        plt.xlabel(x)\n",
    "        plt.ylabel(y)\n",
    "        plt.legend()\n",
    "        plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-18T10:32:06.465543900Z",
     "start_time": "2023-06-18T10:32:06.452517400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def smooth(scalars, weight=0.9, reverse=False):\n",
    "    \"\"\"\n",
    "    TODO\n",
    "    \"\"\"\n",
    "    if reverse:\n",
    "        scalars = list(np.array(scalars)[::-1])\n",
    "    last = scalars[0]\n",
    "    smoothed = list()\n",
    "    for point in scalars:\n",
    "        smoothed_val = last * weight + (1 - weight) * point\n",
    "        smoothed.append(smoothed_val)\n",
    "        last = smoothed_val\n",
    "    if reverse:\n",
    "        smoothed = list(np.array(smoothed)[::-1])\n",
    "    return smoothed"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def custom_line_plots(ids_start, ids_stop, groups, data, x, y, hue,\n",
    "              fs=18, lw=5, x_label=None, y_label=None, k=1, aspect_ratio=0.5,\n",
    "              weight=None, ylim=(0, 100), palette='bright', reverse=True):\n",
    "    nrows = len(ids_start)\n",
    "    with rc_context({'lines.linewidth': lw * k, 'font.size': fs * k}):\n",
    "        fig, axs = plt.subplots(nrows=nrows,\n",
    "                               figsize=(k * 16, k * 16 * aspect_ratio * nrows))\n",
    "        sns.set_style(\"darkgrid\")\n",
    "        for (ax, id_start, id_stop) in zip(axs, ids_start, ids_stop):\n",
    "            data2 = data[data[hue].isin(groups[id_start:id_stop])]\n",
    "            if weight is not None:\n",
    "              for source in groups[id_start:id_stop]:\n",
    "                data2.loc[data2[hue]==source, y] = smooth(data2.loc[data2[hue]==source, y].tolist(), weight=weight,\n",
    "                                                          reverse=reverse)\n",
    "            sns.lineplot(ax=ax, data=data2, x=x, y=y, hue=hue, palette=palette)\n",
    "            if x_label is not None: ax.set_xlabel(x_label)\n",
    "            if y_label is not None: ax.set_ylabel(y_label)\n",
    "            ax.set_ylim(*ylim)\n",
    "        fig.tight_layout()\n",
    "        plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "@F.udf(returnType=T.IntegerType())\n",
    "def get_year(date_in_str):\n",
    "    return int(date_in_str[:4])\n",
    "\n",
    "@F.udf(returnType=T.DatetimeType()) # not sure what to pick\n",
    "def get_year_and_month(date_in_str):\n",
    "    return datetime.datetime.strptime('/'.join(date_in_str.split('/')[:2]), '%Y/%m')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### title"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "main = lem_main_df.select('date_parsed','type','source','title_id')\n",
    "main = main.join(title, main.title_id==title.id, how='left') \\\n",
    "           .drop('title_id', 'id') \\\n",
    "           .withColumn('source_group', sources_to_groups(F.col('source'))) \\\n",
    "           .withColumn('year', get_year(F.col('date_parsed'))) \\\n",
    "           .withColumn('date_parsed', get_year_and_month(F.col('date_parsed')))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "grouped_by_year = grouper(main=main, cols_to_group=['year']).toPandas()\n",
    "grouped_by_source = grouper(main=main, cols_to_group=['source']).toPandas()\n",
    "grouped_by_sourceGroup = grouper(main=main, cols_to_group=['source_group']).toPandas()\n",
    "grouped_by_type = grouper(main=main, cols_to_group=['type'], max_groups=10).toPandas()\n",
    "grouped_by_year_and_source = grouper(main=main, cols_to_group=['source','year']).toPandas()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "grouped_by_year = reshape_df(df=grouped_by_year, cols_to_group=['year'])\n",
    "grouped_by_source = reshape_df(df=grouped_by_year, cols_to_group=['source'])\n",
    "grouped_by_sourceGroup = reshape_df(df=grouped_by_sourceGroup, cols_to_group=['source_group'])\n",
    "grouped_by_type = reshape_df(df=grouped_by_year, cols_to_group=['type'])\n",
    "grouped_by_year_and_source = reshape_df(df=grouped_by_year, cols_to_group=['source','year'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "potential-console",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_bar_plot(data=grouped_by_type, x='type', y='rate', hue='label', fs=18, aspect_ratio=0.4, x_rotation=45, ylim=(0, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "custom_bar_plot(data=grouped_by_year, x='year', y='rate', hue='label', fs=18, aspect_ratio=0.4, x_vals=[k for k in range(2014, 2024)], ylim=(0, 100), show_values=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "custom_bar_plot(data=grouped_by_source, x='source', y='rate', hue='label', fs=18, aspect_ratio=0.4, x_rotation=60, ylim=(0, 100))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "custom_bar_plot(data=grouped_by_sourceGroup, x='group', y='rate', hue='label', k=0.5, fs=18*2, aspect_ratio=1, x_rotation=0, ylim=(0, 100), show_values=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "custom_line_plots(ids_start=[0, 9, 18],\n",
    "                 ids_stop=[9, 18, 26],\n",
    "                 groups=list(grouped_by_year_and_source['source'].unique()),\n",
    "                 data=grouped_by_year_and_source[grouped_by_year_and_source['date_parsed'] >= datetime.datetime.strptime('2014', '%Y')],\n",
    "                 x='date_parsed',\n",
    "                 y='neg_rate',\n",
    "                 hue='source',\n",
    "                 fs=18, lw=5,\n",
    "                 x_label='date parsed',\n",
    "                 y_label='Negativity Rate',\n",
    "                 k=1,\n",
    "                 aspect_ratio=0.3,\n",
    "                 weight=0.9,\n",
    "                 reverse=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### summary"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "main = lem_main_df.select('date_parsed','type','source','summary_id')\n",
    "main = main.join(title, main.summary_id==summary.id, how='left') \\\n",
    "           .dropna(axis=0, subset='summary_id') \\\n",
    "           .drop('summary_id', 'id') \\\n",
    "           .withColumn('source_group', sources_to_groups(F.col('source'))) \\\n",
    "           .withColumn('year', get_year(F.col('date_parsed'))) \\\n",
    "           .withColumn('date_parsed', get_year_and_month(F.col('date_parsed')))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "grouped_by_year = grouper(main=main, cols_to_group=['year']).toPandas()\n",
    "grouped_by_source = grouper(main=main, cols_to_group=['source']).toPandas()\n",
    "grouped_by_sourceGroup = grouper(main=main, cols_to_group=['source_group']).toPandas()\n",
    "grouped_by_type = grouper(main=main, cols_to_group=['type'], max_groups=10).toPandas()\n",
    "grouped_by_year_and_source = grouper(main=main, cols_to_group=['source','year']).toPandas()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "grouped_by_year = reshape_df(df=grouped_by_year, cols_to_group=['year'])\n",
    "grouped_by_source = reshape_df(df=grouped_by_year, cols_to_group=['source'])\n",
    "grouped_by_sourceGroup = reshape_df(df=grouped_by_sourceGroup, cols_to_group=['source_group'])\n",
    "grouped_by_type = reshape_df(df=grouped_by_year, cols_to_group=['type'])\n",
    "grouped_by_year_and_source = reshape_df(df=grouped_by_year, cols_to_group=['source','year'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "custom_bar_plot(data=grouped_by_type, x='type', y='rate', hue='label', fs=18, aspect_ratio=0.4, x_rotation=45, ylim=(0, 100))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "custom_bar_plot(data=grouped_by_year, x='year', y='rate', hue='label', fs=18, aspect_ratio=0.4, x_vals=[k for k in range(2014, 2024)], ylim=(0, 100), show_values=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "custom_bar_plot(data=grouped_by_source, x='source', y='rate', hue='label', fs=18, aspect_ratio=0.4, x_rotation=60, ylim=(0, 100))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "custom_bar_plot(data=grouped_by_sourceGroup, x='group', y='rate', hue='label', k=0.5, fs=18*2, aspect_ratio=1, x_rotation=0, ylim=(0, 100), show_values=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "custom_line_plots(ids_start=[0, 8, 16],\n",
    "                 ids_stop=[8, 16, 24],\n",
    "                 groups=list(grouped_by_year_and_source['source'].unique()),\n",
    "                 data=grouped_by_year_and_source[grouped_by_year_and_source['date_parsed'] >= datetime.datetime.strptime('2014', '%Y')],\n",
    "                 x='date_parsed',\n",
    "                 y='neg_rate',\n",
    "                 hue='source',\n",
    "                 fs=18, lw=5,\n",
    "                 x_label='date parsed',\n",
    "                 y_label='Negativity Rate',\n",
    "                 k=1,\n",
    "                 aspect_ratio=0.3,\n",
    "                 ylim=(-5, 105),\n",
    "                 weight=0.9,\n",
    "                 reverse=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
