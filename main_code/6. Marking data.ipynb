{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o9wKUro3Xlnc"
   },
   "outputs": [],
   "source": [
    "#pip install pip --upgrade >> None\n",
    "#pip install pytorch torchvision torchaudio pytorch-cuda=11.7 -c pytorch -c nvidia\n",
    "#pip install --no-cache-dir transformers sentencepiece >> None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-01T07:24:00.574773Z",
     "start_time": "2023-03-01T07:23:52.675602Z"
    },
    "id": "qQ7TUb2jGIRA"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import ast\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA A100 80GB PCIe'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "PT2RQubQmjeQ"
   },
   "outputs": [],
   "source": [
    "def load_model(model_checkpoint, device='cuda'):\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, return_dict=True).to(device)\n",
    "    clear_output()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "3T_MlOHq3_V-"
   },
   "outputs": [],
   "source": [
    "def load_tokenizer(model_checkpoint, use_fast=True):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=use_fast)\n",
    "    clear_output()\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "dqWCP4eIm1_Y"
   },
   "outputs": [],
   "source": [
    "def predict(data, text_col, model, tokenizer, batch_size, max_length=200, padding=True, truncation=True, return_tensors='pt', device='cuda', order=[0, 1, 2],\n",
    "            exception_max_length=514): \n",
    "  predicted_probs = []\n",
    "  predicted_labels = []\n",
    "  not_null_indexes = data[data[text_col].notnull()].index\n",
    "  data2 = data.loc[not_null_indexes]\n",
    "  lengths = data2[text_col].apply(lambda x: len(x.split(' ')))\n",
    "  with torch.no_grad():\n",
    "    ids = [id*batch_size for id in range(math.ceil(len(data2)/batch_size))] + [len(data2)]\n",
    "    for i in tqdm(range(len(ids)-1)):   \n",
    "      if max_length == 'max_per_batch':\n",
    "        max_l = min(max(lengths[ids[i]:ids[i+1]]), exception_max_length)\n",
    "        inputs = tokenizer(list(data2.iloc[ids[i]:ids[i+1]][text_col].values), max_length=max_l, padding=padding, truncation=truncation, return_tensors=return_tensors).to(device)\n",
    "      else:\n",
    "        inputs = tokenizer(list(data2.iloc[ids[i]:ids[i+1]][text_col].values), max_length=max_length, padding=padding, truncation=truncation, return_tensors=return_tensors).to(device)\n",
    "      outputs = model(**inputs)\n",
    "      probs = torch.nn.functional.softmax(outputs.logits, dim=1).to('cpu')[:, order]\n",
    "      labels = torch.argmax(probs, dim=1).numpy()\n",
    "\n",
    "      predicted_probs.append(probs.numpy())\n",
    "      predicted_labels.append(labels)\n",
    "  return np.vstack(predicted_probs), np.hstack(predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "IV2P0bQiK5S0"
   },
   "outputs": [],
   "source": [
    "def merge(data, probs, labels, id_of_model):\n",
    "  data[[f'negative_{id_of_model}',f'neutral_{id_of_model}',f'positive_{id_of_model}']] = probs\n",
    "  data[f'pred_label_{id_of_model}'] = labels\n",
    "  return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "__NfdTN0MCLB"
   },
   "outputs": [],
   "source": [
    "def predictions_to_df(probs, labels, id_of_model, text_type, data, df=None):\n",
    "  if df is None:\n",
    "    df = pd.DataFrame()\n",
    "    df['index'] = np.arange(len(data))\n",
    "    not_null_indexes = data[data[text_type].notnull()].index\n",
    "    df.loc[not_null_indexes, [f'negative_{id_of_model}_{text_type}',f'neutral_{id_of_model}_{text_type}',f'positive_{id_of_model}_{text_type}']] = probs\n",
    "    df.loc[not_null_indexes, f'pred_label_{id_of_model}_{text_type}'] = labels\n",
    "    df = df.drop('index', axis=1)\n",
    "  else:\n",
    "    not_null_indexes = data[data[text_type].notnull()].index\n",
    "    df.loc[not_null_indexes, [f'negative_{id_of_model}_{text_type}',f'neutral_{id_of_model}_{text_type}',f'positive_{id_of_model}_{text_type}']] = probs\n",
    "    df.loc[not_null_indexes, f'pred_label_{id_of_model}_{text_type}'] = labels\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "2EclHP2eK2il"
   },
   "outputs": [],
   "source": [
    "def save_data(data, path, filename):\n",
    "    if filename.split('.')[-1:][0] == 'csv':\n",
    "        data.to_csv(os.path.join(path, filename), index=False)\n",
    "    elif filename.split('.')[-1:][0] == 'json':\n",
    "        data.to_json(os.path.join(path, filename))\n",
    "    elif filename.split('.')[-1:][0] == 'xlsx':\n",
    "        data.to_excel(os.path.join(path, filename), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = pd.read_csv(\"/home/ubuntu/summary.csv\")\n",
    "data2 = pd.read_csv(\"/home/ubuntu/title.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reform(x):\n",
    "    try:\n",
    "        return ' '.join(ast.literal_eval(x))\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d37941fb6d744889a75567824b2df67f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3737450 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data1['text'] = list(tqdm(map(lambda x: reform(x), data1['text']), total=len(data1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data2['text'] = list(tqdm(map(lambda x: reform(x), data2['text']), total=len(data2)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# sismetanin/sbert-ru-sentiment-krnd"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_checkpoint = \"sismetanin/sbert-ru-sentiment-krnd\"\n",
    "model = load_model(model_checkpoint, 'cuda')\n",
    "tokenizer = load_tokenizer(model_checkpoint, use_fast=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Summary"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "44d323144db24a80a30afd7c9a9607d8",
      "856d0affabd747ddb7fc216e0fe6cf56",
      "a91b030b92744192b1a6a2023b214ac7",
      "d4db8bf8a1ba4b61b81f21e4e9374a82",
      "0e1f137dfd754617851d6e0cbdc7410c",
      "e0d3d5f15ada4f33a88441a3e35c401d",
      "6a60351d759d466a9e34bfdc00e6c50b",
      "22d3b7e1784544a4a77452023d7d796d",
      "bc022deca88541faa373ce264fa2c899",
      "d2cc8977281f4ac7988ba797d362a092",
      "fb0d3d736ab845a0b624ac5f6565bc9c"
     ]
    },
    "executionInfo": {
     "elapsed": 3995219,
     "status": "ok",
     "timestamp": 1686498185821,
     "user": {
      "displayName": "Egor Udalov",
      "userId": "00848787650991375786"
     },
     "user_tz": -180
    },
    "id": "k1nrDG3E3Wih",
    "outputId": "f59dc6da-bb6d-4b8b-b614-e5e6bf638874",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef80b2333ab9428aa559357d05598571",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/29199 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "probs1, labels1 = predict(data1, 'text', model, tokenizer, batch_size=128, max_length='max_per_batch',\n",
    "                          padding=True, truncation=True, return_tensors='pt', device='cuda', order=[0, 1, 2],\n",
    "                          exception_max_length=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df1 = predictions_to_df(probs1, labels1, id_of_model='', text_type='text', data=data1, df=None)\n",
    "df1['id'] = data1['id']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "save_data(df1, path='/home/ubuntu/', filename='summary_pred.csv')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Title"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs2, labels2 = predict(data2, 'text', model, tokenizer, batch_size=100, max_length='max_per_batch',\n",
    "                          padding=True, truncation=True, return_tensors='pt', device='cuda', order=[0, 1, 2],\n",
    "                          exception_max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df2 = predictions_to_df(probs2, labels2, id_of_model='', text_type='text', data=data2, df=None)\n",
    "df2['id'] = data2['id']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_data(df2, path='/home/ubuntu/', filename='title_pred.csv')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPxIBCFsI+va+AiCh4fwNzq",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
